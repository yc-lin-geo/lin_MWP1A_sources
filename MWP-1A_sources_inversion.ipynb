{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modules\n",
    "import numpy as np #version 1.18.1\n",
    "import pandas as pd #version 1.0.1\n",
    "from scipy.optimize import nnls #version 1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load RSL rise magnitude at each site\n",
    "#------------------Empirical Senario------------------\n",
    "Emp_mag = pd.read_excel('RSL magnitude/Lin et al., Sup data.xlsx',sheet_name=1) \n",
    "Tahiti_emp_dis = Emp_mag['Tahiti']\n",
    "Tahiti_emp_std = np.std(Tahiti_emp_dis)\n",
    "Barbados_emp_dis =  Emp_mag['Barbados']\n",
    "Barbados_emp_std = np.std(Barbados_emp_dis)\n",
    "Sunda_emp_dis =  Emp_mag['Sunda Shelf']\n",
    "Sunda_emp_std = np.std(Sunda_emp_dis)\n",
    "HYD_emp_dis =  Emp_mag['Hydrographer\\'s Passage (HYD)']\n",
    "HYD_emp_std = np.std(HYD_emp_dis)\n",
    "NOG_emp_dis =  Emp_mag['Noggin Pass (NOG)']\n",
    "NOG_emp_std = np.std(NOG_emp_dis)\n",
    "Scot_emp_dis =  Emp_mag['Northwest Scotland']\n",
    "Scot_emp_std = np.std(Scot_emp_dis)\n",
    "total_emp_std = np.array([Tahiti_emp_std,Barbados_emp_std,Sunda_emp_std,HYD_emp_std,NOG_emp_std,\n",
    "                        Scot_emp_std]) \n",
    "#------------------Uniform Senario------------------\n",
    "Uni_mag = pd.read_excel('RSL magnitude/Lin et al., Sup data.xlsx',sheet_name=2) #Uniform Senario\n",
    "Tahiti_uni_dis = Uni_mag['Tahiti']\n",
    "Tahiti_uni_std = np.std(Tahiti_uni_dis)\n",
    "Barbados_uni_dis =  Uni_mag['Barbados']\n",
    "Barbados_uni_std = np.std(Barbados_uni_dis)\n",
    "Sunda_uni_dis =  Uni_mag['Sunda Shelf']\n",
    "Sunda_uni_std = np.std(Sunda_uni_dis)\n",
    "HYD_uni_dis =  Uni_mag['Hydrographer\\'s Passage (HYD)']\n",
    "HYD_uni_std = np.std(HYD_uni_dis)\n",
    "NOG_uni_dis =  Uni_mag['Noggin Pass (NOG)']\n",
    "NOG_uni_std = np.std(NOG_uni_dis)\n",
    "Scot_uni_dis =  Uni_mag['Northwest Scotland']\n",
    "Scot_uni_std = np.std(Scot_uni_dis)\n",
    "total_uni_std = np.array([Tahiti_uni_std,Barbados_uni_std,Sunda_uni_std,HYD_uni_std,NOG_uni_std,\n",
    "                        Scot_uni_std])\n",
    "#load sea-level fingerprint matrix\n",
    "#each row corresponds to each sea-level site and each column corresponds to each ice sheet\n",
    "A =  np.array(pd.read_csv('RSL magnitude/fingerprint.csv',index_col=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximum_prob(values,percent):\n",
    "    '''This function is used to analytically find the smallest confidence interval  \n",
    "    range from a data ensemble \n",
    "    values:  data samples\n",
    "    percent: probablity range to find (e.g., 95 is 95% CI)'''\n",
    "    \n",
    "    v_max,v_min = np.max(values),np.min(values)\n",
    "    ranges = []\n",
    "    for i in np.linspace(0,100-percent,1000):\n",
    "        _b,_e = np.percentile(values,[i,i+percent])\n",
    "        ranges.append([_b,_e,np.abs(_b-_e),i,i+percent])\n",
    "    ranges = np.array(ranges)\n",
    "    min_index = np.argmin(ranges[:,2])\n",
    "    \n",
    "    return ranges[min_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MWP-1A Sources Inversion\n",
    "\n",
    "All loaded 20,000 MWP-1A RSL rise magnitudes samples represent the elastic induced relative sea-level rise at each location, which can be used to invert the MWP-1A sources following the equation:\n",
    "\n",
    "\n",
    "\n",
    "$$ \\Delta {RSL}_{Elastic}(\\varphi)  = {ESL}_{NAIS}  \\times F_{NAIS}(\\varphi) + {ESL}_{AIS} \\times F_{AIS}(\\varphi) +{ESL}_{SIS}\\times F_{SIS}(\\varphi)$$\n",
    "\n",
    "where $\\Delta {RSL}_{Elastic}(\\varphi)$ represent the instantaneous response of the solid Earth and sea surface to an influx of meltwater at location $\\varphi$; three $ESL$ terms represent eustatic sea-level (ESL) contributions from the NAIS, AIS, and SIS ice sheets and the $F(\\varphi)$ terms denote ice-sheet-specific, site-specific sea-level fingerprint values.\n",
    "\n",
    "\n",
    "By optimising this equation above using the weighted non-negative least sqaure approch based on Lawson-Hanson algorithm (from a [scipy](https://docs.scipy.org/doc/scipy/reference/index.html) function [nnls](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html)), three optimised $ESL$ paramters in the equation above can be obtained. \n",
    "\n",
    "### Empirical senario for coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -\n",
      "----------------Mean Result (95% confidence interval)----------------\n",
      "NAIS: 12.280 [0.000,18.565]\n",
      "AIS: 2.532 [0.000,10.692]\n",
      "SIS:   3.490 [0.000,6.565]\n",
      "Total MWP-1A Magnitude:   18.302 [14.485,22.068]\n"
     ]
    }
   ],
   "source": [
    "emp_result = []\n",
    "while len(emp_result)<20000:\n",
    "    i = np.random.randint(0,20000,1)[0]\n",
    "    obs = np.array([Tahiti_emp_dis.iloc[i],Barbados_emp_dis.iloc[i],Sunda_emp_dis.iloc[i],HYD_emp_dis.iloc[i],\n",
    "                   NOG_emp_dis.iloc[i],Scot_emp_dis.iloc[i]]) #compile all sample together\n",
    "    opti = list(nnls(np.sqrt(1/np.array(total_emp_std)**2)[:,None] * A,np.sqrt(1/np.array(total_emp_std)**2)*obs)[0]) #non-negative least square\n",
    "    emp_result.append(opti) #append results from this sample\n",
    "    \n",
    "emp_result= np.array(emp_result)\n",
    "print('----------------MWP1A Magnitude & Sources with Empirical Distribution -')\n",
    "print('----------------Mean Result (95% confidence interval)----------------')\n",
    "print('NAIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,0]),*find_maximum_prob(emp_result[:,0],95)[:2]))\n",
    "print('AIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,1]),*find_maximum_prob(emp_result[:,1],95)[:2]))\n",
    "print('SIS:   {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,2]),*find_maximum_prob(emp_result[:,2],95)[:2]))\n",
    "print('Total MWP-1A Magnitude:   {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(np.sum(emp_result,axis=1)),*find_maximum_prob(np.sum(emp_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform senario coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -\n",
      "----------------Mean Result (95% confidence interval)----------------\n",
      "NAIS: 12.428 [3.512,19.609]\n",
      "AIS: 2.164 [0.000,8.692]\n",
      "SIS:   3.124 [0.000,5.953]\n",
      "Total MWP-1A Magnitude:   17.717 [14.902,20.600]\n"
     ]
    }
   ],
   "source": [
    "uni_result = []\n",
    "while len(uni_result)<20000:\n",
    "    i = np.random.randint(0,20000,1)[0]\n",
    "    obs = np.array([Tahiti_uni_dis.iloc[i],Barbados_uni_dis.iloc[i],Sunda_uni_dis.iloc[i],HYD_uni_dis.iloc[i],\n",
    "                   NOG_uni_dis.iloc[i],Scot_uni_dis.iloc[i]]) #compile all sample together\n",
    "    opti = list(nnls(np.sqrt(1/np.array(total_uni_std)**2)[:,None] * A,np.sqrt(1/np.array(total_uni_std)**2)*obs)[0]) #non-negative least square\n",
    "    uni_result.append(opti)\n",
    "    \n",
    "uni_result= np.array(uni_result)\n",
    "print('----------------MWP1A Magnitude & Sources with Uniform Distribution -')\n",
    "print('----------------Mean Result (95% confidence interval)----------------')\n",
    "print('NAIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,0]),*find_maximum_prob(uni_result[:,0],95)[:2]))\n",
    "print('AIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,1]),*find_maximum_prob(uni_result[:,1],95)[:2]))\n",
    "print('SIS:   {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,2]),*find_maximum_prob(uni_result[:,2],95)[:2]))\n",
    "print('Total MWP-1A Magnitude:   {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(np.sum(uni_result,axis=1)),*find_maximum_prob(np.sum(uni_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Jackknife Resampling\n",
    "\n",
    "### Empirical senario for coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Without Tahiti -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 11.395 [0.000,19.050]  (6.207)\n",
      "West Antarctic Ice Sheet: 3.058 [0.000,11.904] (4.397)\n",
      "Scandinavian Ice Sheet:   3.479 [0.000,6.568]  (2.029)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Barbados -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 7.508 [0.000,17.911]  (7.232)\n",
      "West Antarctic Ice Sheet: 6.912 [0.000,15.973] (6.288)\n",
      "Scandinavian Ice Sheet:   5.156 [0.000,10.001]  (2.932)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Sunda Shelf -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.548 [0.000,18.763]  (5.002)\n",
      "West Antarctic Ice Sheet: 2.375 [0.000,10.458] (3.600)\n",
      "Scandinavian Ice Sheet:   3.511 [0.000,6.624]  (2.035)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Hydrographer -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 11.640 [0.000,18.330]  (5.354)\n",
      "West Antarctic Ice Sheet: 2.880 [0.000,11.058] (3.892)\n",
      "Scandinavian Ice Sheet:   3.418 [0.000,6.521]  (2.012)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Noggin Pass -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.033 [0.000,18.432]  (5.113)\n",
      "West Antarctic Ice Sheet: 2.631 [0.000,10.740] (3.711)\n",
      "Scandinavian Ice Sheet:   3.478 [0.000,6.534]  (2.018)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Scotland -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 13.839 [0.000,21.032]  (6.244)\n",
      "West Antarctic Ice Sheet: 2.979 [0.000,13.432] (4.540)\n",
      "Scandinavian Ice Sheet:   1.480 [0.000,11.200]  (3.773)\n",
      "----------------------------------------------------------------------\n",
      "----------------Overall Jackknife/Original Results --------------------\n",
      "NAIS: 11.494, 12.280  Bias: -0.786 Bias Corrected: 13.066 m\n",
      "AIS: 3.472, 2.532  Bias: 0.941 Bias Corrected: 1.591 m\n",
      "SIS:   3.420, 3.490  Bias: -0.070 Bias Corrected: 3.561 m\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "jack_knife_result =np.zeros([60000,3])\n",
    "site_name = ['Tahiti',\"Barbados\",'Sunda Shelf','Hydrographer','Noggin Pass',\n",
    "            'Scotland']\n",
    "for jack in range(6):\n",
    "    \n",
    "    all_emp_result = [Tahiti_emp_dis,Barbados_emp_dis,Sunda_emp_dis,NOG_emp_dis,\n",
    "                           HYD_emp_dis,Scot_emp_dis]\n",
    "    total_emp_std = [Tahiti_emp_std,Barbados_emp_std,Sunda_emp_std,HYD_emp_std,NOG_emp_std,\n",
    "                         Scot_emp_std]\n",
    "    A_index = ~(np.arange(0,6)==jack)\n",
    "    A_jack = A[A_index]\n",
    "\n",
    "    del all_emp_result[jack] #remove one site's observation\n",
    "    del total_emp_std[jack] #remove one site's standard deviation \n",
    "    emp_test_result = []\n",
    "    all_emp_result = np.array(all_emp_result)\n",
    "    total_emp_std = np.array(total_emp_std)\n",
    "    while len(emp_test_result)<10000:\n",
    "        \n",
    "        i = np.random.randint(0,20000,1)[0]\n",
    "        obs = np.array([Tahiti_emp_dis.iloc[i],Barbados_emp_dis.iloc[i],Sunda_emp_dis.iloc[i],HYD_emp_dis.iloc[i],\n",
    "                       NOG_emp_dis.iloc[i],Scot_emp_dis.iloc[i]])\n",
    "        obs = obs[A_index] \n",
    "        opti = list(nnls(np.sqrt(1/np.array(total_emp_std)**2)[:,None] * A_jack,np.sqrt(1/np.array(total_emp_std)**2)*obs)[0])\n",
    "        emp_test_result.append(opti)\n",
    "       \n",
    "    \n",
    "    emp_test_result= np.array(emp_test_result)\n",
    "    jack_knife_result[jack*10000:(jack+1)*10000,:]=emp_test_result\n",
    "    print('----------------Without {:} -------------------------------------'.format(site_name[jack]))\n",
    "    print('----------------MWP1A Magnitude & Sources with Empirical Distribution -----------')\n",
    "    print('----------------Mean Result [95% confidence interval] (1 std)-----------------')\n",
    "    print('North American Ice Sheet: {0:5.3f} [{1:5.3f},{2:5.3f}]  ({3:5.3f})'.format(np.mean(emp_test_result[:,0]),find_maximum_prob(emp_test_result[:,0],95)[0],find_maximum_prob(emp_test_result[:,0],95)[1],\n",
    "                                                                                   np.std(emp_test_result[:,0])))\n",
    "    print('West Antarctic Ice Sheet: {0:5.3f} [{1:5.3f},{2:5.3f}] ({3:5.3f})'.format(np.mean(emp_test_result[:,1]),find_maximum_prob(emp_test_result[:,1],95)[0],find_maximum_prob(emp_test_result[:,1],95)[1],\n",
    "                                                                                  np.std(emp_test_result[:,1])))\n",
    "    print('Scandinavian Ice Sheet:   {0:5.3f} [{1:5.3f},{2:5.3f}]  ({3:5.3f})'.format(np.mean(emp_test_result[:,2]),find_maximum_prob(emp_test_result[:,2],95)[0],find_maximum_prob(emp_test_result[:,2],95)[1],\n",
    "                                                                                  np.std(emp_test_result[:,2])))\n",
    "    print('----------------------------------------------------------------------')\n",
    "\n",
    "print('----------------Overall Jackknife/Original Results --------------------')\n",
    "print('NAIS: {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,0]),np.mean(emp_result[:,0]),\n",
    "                                                                           np.mean(jack_knife_result[:,0])-np.mean(emp_result[:,0]),\n",
    "                                                                            np.mean(emp_result[:,0])-(np.mean(jack_knife_result[:,0])-np.mean(emp_result[:,0]))))\n",
    "print('AIS: {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,1]),np.mean(emp_result[:,1]),\n",
    "                                                                           np.mean(jack_knife_result[:,1])-np.mean(emp_result[:,1]),\n",
    "                                                                           np.mean(emp_result[:,1])-(np.mean(jack_knife_result[:,1])-np.mean(emp_result[:,1]))))\n",
    "print('SIS:   {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,2]),np.mean(emp_result[:,2]),\n",
    "                                                                          np.mean(jack_knife_result[:,2])-np.mean(emp_result[:,2]),\n",
    "                                                                        np.mean(emp_result[:,2])-( np.mean(jack_knife_result[:,2])-np.mean(emp_result[:,2]))))\n",
    "print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform senario coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Without Tahiti -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 10.562 [0.000,18.263]  (4.184)\n",
      "West Antarctic Ice Sheet: 3.318 [0.000,11.440] (4.276)\n",
      "Scandinavian Ice Sheet:   3.221 [0.000,5.981]  (1.887)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Barbados -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 8.714 [0.000,17.220]  (4.184)\n",
      "West Antarctic Ice Sheet: 5.547 [0.000,15.232] (5.946)\n",
      "Scandinavian Ice Sheet:   4.379 [0.000,8.760]  (2.622)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Sunda Shelf -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.576 [3.528,19.371]  (4.184)\n",
      "West Antarctic Ice Sheet: 2.089 [0.000,8.651] (2.979)\n",
      "Scandinavian Ice Sheet:   3.119 [0.000,5.941]  (1.878)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Hydrographer -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.306 [2.859,19.113]  (4.184)\n",
      "West Antarctic Ice Sheet: 2.218 [0.000,8.838] (3.083)\n",
      "Scandinavian Ice Sheet:   3.136 [0.000,5.930]  (1.867)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Noggin Pass -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.249 [1.928,18.781]  (4.184)\n",
      "West Antarctic Ice Sheet: 2.268 [0.000,9.141] (3.134)\n",
      "Scandinavian Ice Sheet:   3.134 [0.000,5.970]  (1.889)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Scotland -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 13.695 [3.110,20.788]  (4.184)\n",
      "West Antarctic Ice Sheet: 2.212 [0.000,9.876] (3.483)\n",
      "Scandinavian Ice Sheet:   1.845 [0.000,11.070]  (3.811)\n",
      "----------------------------------------------------------------------\n",
      "----------------Overall Jackknife/Original Results --------------------\n",
      "NAIS: 11.684, 12.428  Bias: -0.745 Bias Corrected: 13.173 m\n",
      "AIS: 2.942, 2.164  Bias: 0.778 Bias Corrected: 1.386 m\n",
      "SIS:   3.139, 3.124  Bias: 0.015 Bias Corrected: 3.110 m\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "jack_knife_result =np.zeros([60000,3])\n",
    "site_name = ['Tahiti',\"Barbados\",'Sunda Shelf','Hydrographer','Noggin Pass',\n",
    "            'Scotland']\n",
    "for jack in range(6):\n",
    "    \n",
    "    all_uni_result = [Tahiti_uni_dis,Barbados_uni_dis,Sunda_uni_dis,NOG_uni_dis,\n",
    "                           HYD_uni_dis,Scot_uni_dis]\n",
    "    total_uni_std = [Tahiti_uni_std,Barbados_uni_std,Sunda_uni_std,HYD_uni_std,NOG_uni_std,\n",
    "                         Scot_uni_std]\n",
    "    A_index = ~(np.arange(0,6)==jack)\n",
    "    A_jack = A[A_index]\n",
    "\n",
    "    del all_uni_result[jack] #remove one site's observation\n",
    "    del total_uni_std[jack] #remove one site's standard error \n",
    "    uni_test_result = []\n",
    "    all_uni_result = np.array(all_uni_result)\n",
    "    total_uni_std = np.array(total_uni_std)\n",
    "    while len(uni_test_result)<10000:\n",
    "        \n",
    "        i = np.random.randint(0,20000,1)[0]\n",
    "        obs = np.array([Tahiti_uni_dis.iloc[i],Barbados_uni_dis.iloc[i],Sunda_uni_dis.iloc[i],HYD_uni_dis.iloc[i],\n",
    "                       NOG_uni_dis.iloc[i],Scot_uni_dis.iloc[i]])\n",
    "        obs = obs[A_index] \n",
    "        opti = list(nnls(np.sqrt(1/np.array(total_uni_std)**2)[:,None] * A_jack,np.sqrt(1/np.array(total_uni_std)**2)*obs)[0])\n",
    "        uni_test_result.append(opti)\n",
    "       \n",
    "    \n",
    "    uni_test_result= np.array(uni_test_result)\n",
    "    jack_knife_result[jack*10000:(jack+1)*10000,:]=uni_test_result\n",
    "    print('----------------Without {:} -------------------------------------'.format(site_name[jack]))\n",
    "    print('----------------MWP1A Magnitude & Sources with Uniform Distribution -----------')\n",
    "    print('----------------Mean Result [95% confidence interval] (1 std)-----------------')\n",
    "    print('North American Ice Sheet: {0:5.3f} [{1:5.3f},{2:5.3f}]  ({3:5.3f})'.format(np.mean(uni_test_result[:,0]),find_maximum_prob(uni_test_result[:,0],95)[0],find_maximum_prob(uni_test_result[:,0],95)[1],\n",
    "                                                                                   np.std(uni_result[:,0])))\n",
    "    print('West Antarctic Ice Sheet: {0:5.3f} [{1:5.3f},{2:5.3f}] ({3:5.3f})'.format(np.mean(uni_test_result[:,1]),find_maximum_prob(uni_test_result[:,1],95)[0],find_maximum_prob(uni_test_result[:,1],95)[1],\n",
    "                                                                                  np.std(uni_test_result[:,1])))\n",
    "    print('Scandinavian Ice Sheet:   {0:5.3f} [{1:5.3f},{2:5.3f}]  ({3:5.3f})'.format(np.mean(uni_test_result[:,2]),find_maximum_prob(uni_test_result[:,2],95)[0],find_maximum_prob(uni_test_result[:,2],95)[1],\n",
    "                                                                                  np.std(uni_test_result[:,2])))\n",
    "    print('----------------------------------------------------------------------')\n",
    "\n",
    "print('----------------Overall Jackknife/Original Results --------------------')\n",
    "print('NAIS: {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,0]),np.mean(uni_result[:,0]),\n",
    "                                                                           np.mean(jack_knife_result[:,0])-np.mean(uni_result[:,0]),\n",
    "                                                                            np.mean(uni_result[:,0])-(np.mean(jack_knife_result[:,0])-np.mean(uni_result[:,0]))))\n",
    "print('AIS: {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,1]),np.mean(uni_result[:,1]),\n",
    "                                                                           np.mean(jack_knife_result[:,1])-np.mean(uni_result[:,1]),\n",
    "                                                                           np.mean(uni_result[:,1])-(np.mean(jack_knife_result[:,1])-np.mean(uni_result[:,1]))))\n",
    "print('SIS:   {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,2]),np.mean(uni_result[:,2]),\n",
    "                                                                          np.mean(jack_knife_result[:,2])-np.mean(uni_result[:,2]),\n",
    "                                                                        np.mean(uni_result[:,2])-( np.mean(jack_knife_result[:,2])-np.mean(uni_result[:,2]))))\n",
    "print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MWP-1A Sources Inversion with sea-level oscillation limit\n",
    "\n",
    "### Empirical senario for coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result (95% confidence interval)----------------\n",
      "NAIS: 11.283 [0.000,15.666]\n",
      "AIS:  2.066 [0.000,9.422]\n",
      "SIS:  4.939 [2.859,7.595]\n",
      "Total MWP-1A Magnitude:    18.289 [14.624,21.957]\n"
     ]
    }
   ],
   "source": [
    "emp_result = []\n",
    "total_emp_std = np.array([Tahiti_emp_std,Barbados_emp_std,Sunda_emp_std,HYD_emp_std,NOG_emp_std,\n",
    "                        Scot_emp_std]) \n",
    "while len(emp_result)<20000:\n",
    "    i = np.random.randint(0,20000,1)[0]\n",
    "    obs = np.array([Tahiti_emp_dis.iloc[i],Barbados_emp_dis.iloc[i],Sunda_emp_dis.iloc[i],HYD_emp_dis.iloc[i],\n",
    "                   NOG_emp_dis.iloc[i],Scot_emp_dis.iloc[i]]) #compile all sample together\n",
    "    opti = list(nnls(np.sqrt(1/np.array(total_emp_std)**2)[:,None] * A,np.sqrt(1/np.array(total_emp_std)**2)*obs)[0]) #non-negative least square\n",
    "    if (opti[0]*0.75+opti[1]*1.09-opti[2]*0.74)<9: # filter out the results producing a sea-level oscillation\n",
    "        emp_result.append(opti)\n",
    "    \n",
    "emp_result= np.array(emp_result)\n",
    "print('----------------MWP1A Magnitude & Sources with Empirical Distribution -----------')\n",
    "print('----------------Mean Result (95% confidence interval)----------------')\n",
    "print('NAIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,0]),*find_maximum_prob(emp_result[:,0],95)[:2]))\n",
    "print('AIS:  {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,1]),*find_maximum_prob(emp_result[:,1],95)[:2]))\n",
    "print('SIS:  {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,2]),*find_maximum_prob(emp_result[:,2],95)[:2]))\n",
    "print('Total MWP-1A Magnitude:    {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(np.sum(emp_result,axis=1)),*find_maximum_prob(np.sum(emp_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform senario coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result (95% confidence interval)----------------\n",
      "NAIS: 11.493 [3.355,16.077]\n",
      "AIS:  1.709 [0.000,7.373]\n",
      "SIS:  4.561 [2.580,6.578]\n",
      "Total MWP-1A Magnitude: 17.751 [15.078,20.412]\n"
     ]
    }
   ],
   "source": [
    "uni_result = []\n",
    "total_uni_std = np.array([Tahiti_uni_std,Barbados_uni_std,Sunda_uni_std,HYD_uni_std,NOG_uni_std,\n",
    "                        Scot_uni_std])\n",
    "while len(uni_result)<20000:\n",
    "    i = np.random.randint(0,20000,1)[0]\n",
    "    obs = np.array([Tahiti_uni_dis.iloc[i],Barbados_uni_dis.iloc[i],Sunda_uni_dis.iloc[i],HYD_uni_dis.iloc[i],\n",
    "                   NOG_uni_dis.iloc[i],Scot_uni_dis.iloc[i]]) #compile all sample together\n",
    "    opti = list(nnls(np.sqrt(1/np.array(total_uni_std)**2)[:,None] * A,np.sqrt(1/np.array(total_uni_std)**2)*obs)[0]) #non-negative least square\n",
    "    if (opti[0]*0.75+opti[1]*1.09-opti[2]*0.74)<9: # filter out the results producing a sea-level oscillation\n",
    "\n",
    "        uni_result.append(opti)\n",
    "    \n",
    "uni_result= np.array(uni_result)\n",
    "print('----------------MWP1A Magnitude & Sources with Uniform Distribution -----------')\n",
    "print('----------------Mean Result (95% confidence interval)----------------')\n",
    "print('NAIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,0]),*find_maximum_prob(uni_result[:,0],95)[:2]))\n",
    "print('AIS:  {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,1]),*find_maximum_prob(uni_result[:,1],95)[:2]))\n",
    "print('SIS:  {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,2]),*find_maximum_prob(uni_result[:,2],95)[:2]))\n",
    "print('Total MWP-1A Magnitude: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.median(np.sum(uni_result,axis=1)),*find_maximum_prob(np.sum(uni_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Averaged MWP1A Magnitude & Sources -----------\n",
      "----------------Median Result (95% confidence interval)----------------\n",
      "NAIS: 12.107 [5.752,15.478]\n",
      "AIS:  1.283 [0.000,5.872]\n",
      "SIS:  4.723 [3.190,6.315]\n",
      "Total MWP-1A Magnitude: 18.039 [15.078,20.412]\n"
     ]
    }
   ],
   "source": [
    "print('----------------Averaged MWP1A Magnitude & Sources -----------')\n",
    "print('----------------Median Result (95% confidence interval)----------------')\n",
    "print('NAIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.median((uni_result[:,0]+emp_result[:,0])/2),*find_maximum_prob((uni_result[:,0]+emp_result[:,0])/2,95)[:2]))\n",
    "print('AIS:  {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.median((uni_result[:,1]+emp_result[:,1])/2),*find_maximum_prob((uni_result[:,1]+emp_result[:,1])/2,95)[:2]))\n",
    "print('SIS:  {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.median((uni_result[:,2]+emp_result[:,2])/2),*find_maximum_prob((uni_result[:,2]+emp_result[:,2])/2,95)[:2]))\n",
    "print('Total MWP-1A Magnitude: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.median((np.sum(uni_result,axis=1)+np.sum(emp_result,axis=1))/2),*find_maximum_prob(np.sum(uni_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final notes:\n",
    "\n",
    "- Due to the different ramdom seeds, this code can possibly result in a slightly different results comparing to the results shown in our manuscript\n",
    "- If you have any questions, feedbacks or comments, please feel free to contact [the corresponding author](yc-lin.com): yucheng.lin@durham.ac.uk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
