{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Basic Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load modules\n",
    "import numpy as np #version 1.18.1\n",
    "import pandas as pd #version 1.0.1\n",
    "from scipy.optimize import nnls #version 1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load RSL rise magnitude at each site\n",
    "#------------------Empirical Senario------------------\n",
    "Emp_mag = pd.read_excel('RSL magnitude/Lin et al., Sup data.xlsx',sheet_name=1) \n",
    "Tahiti_emp_dis = Emp_mag['Tahiti']\n",
    "Tahiti_emp_std = np.std(Tahiti_emp_dis)\n",
    "Barbados_emp_dis =  Emp_mag['Barbados']\n",
    "Barbados_emp_std = np.std(Barbados_emp_dis)\n",
    "Sunda_emp_dis =  Emp_mag['Sunda Shelf']\n",
    "Sunda_emp_std = np.std(Sunda_emp_dis)\n",
    "HYD_emp_dis =  Emp_mag['Hydrographer\\'s Passage (HYD)']\n",
    "HYD_emp_std = np.std(HYD_emp_dis)\n",
    "NOG_emp_dis =  Emp_mag['Noggin Pass (NOG)']\n",
    "NOG_emp_std = np.std(NOG_emp_dis)\n",
    "Scot_emp_dis =  Emp_mag['Northwest Scotland']\n",
    "Scot_emp_std = np.std(Scot_emp_dis)\n",
    "total_emp_std = np.array([Tahiti_emp_std,Barbados_emp_std,Sunda_emp_std,HYD_emp_std,NOG_emp_std,\n",
    "                        Scot_emp_std]) \n",
    "#------------------Uniform Senario------------------\n",
    "Uni_mag = pd.read_excel('RSL magnitude/Lin et al., Sup data.xlsx',sheet_name=2) #Uniform Senario\n",
    "Tahiti_uni_dis = Uni_mag['Tahiti']\n",
    "Tahiti_uni_std = np.std(Tahiti_uni_dis)\n",
    "Barbados_uni_dis =  Uni_mag['Barbados']\n",
    "Barbados_uni_std = np.std(Barbados_uni_dis)\n",
    "Sunda_uni_dis =  Uni_mag['Sunda Shelf']\n",
    "Sunda_uni_std = np.std(Sunda_uni_dis)\n",
    "HYD_uni_dis =  Uni_mag['Hydrographer\\'s Passage (HYD)']\n",
    "HYD_uni_std = np.std(HYD_uni_dis)\n",
    "NOG_uni_dis =  Uni_mag['Noggin Pass (NOG)']\n",
    "NOG_uni_std = np.std(NOG_uni_dis)\n",
    "Scot_uni_dis =  Uni_mag['Northwest Scotland']\n",
    "Scot_uni_std = np.std(Scot_uni_dis)\n",
    "total_uni_std = np.array([Tahiti_uni_std,Barbados_uni_std,Sunda_uni_std,HYD_uni_std,NOG_uni_std,\n",
    "                        Scot_uni_std])\n",
    "#load sea-level fingerprint matrix\n",
    "#each row corresponds to each sea-level site and each column corresponds to each ice sheet\n",
    "A =  np.array(pd.read_csv('RSL magnitude/fingerprint.csv',index_col=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_maximum_prob(values,percent):\n",
    "    '''This function is used to analytically find the smallest confidence interval  \n",
    "    range from a data ensemble \n",
    "    values:  data samples\n",
    "    percent: probablity range to find (e.g., 95 is 95% CI)'''\n",
    "    \n",
    "    v_max,v_min = np.max(values),np.min(values)\n",
    "    ranges = []\n",
    "    for i in np.linspace(0,100-percent,1000):\n",
    "        _b,_e = np.percentile(values,[i,i+percent])\n",
    "        ranges.append([_b,_e,np.abs(_b-_e),i,i+percent])\n",
    "    ranges = np.array(ranges)\n",
    "    min_index = np.argmin(ranges[:,2])\n",
    "    \n",
    "    return ranges[min_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MWP-1A Sources Inversion\n",
    "\n",
    "### Empirical senario for coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -\n",
      "----------------Mean Result (95% confidence interval)----------------\n",
      "NAIS: 12.253 [0.000,18.607]\n",
      "AIS: 2.551 [0.000,10.661]\n",
      "SIS:   3.481 [0.000,6.584]\n",
      "Total MWP-1A Magnitude:   18.285 [14.304,21.956]\n"
     ]
    }
   ],
   "source": [
    "emp_result = []\n",
    "while len(emp_result)<20000:\n",
    "    i = np.random.randint(0,20000,1)[0]\n",
    "    obs = np.array([Tahiti_emp_dis.iloc[i],Barbados_emp_dis.iloc[i],Sunda_emp_dis.iloc[i],HYD_emp_dis.iloc[i],\n",
    "                   NOG_emp_dis.iloc[i],Scot_emp_dis.iloc[i]]) #compile all sample together\n",
    "    opti = list(nnls(np.sqrt(1/np.array(total_emp_std)**2)[:,None] * A,np.sqrt(1/np.array(total_emp_std)**2)*obs)[0]) #non-negative least square\n",
    "    emp_result.append(opti) #append results from this sample\n",
    "    \n",
    "emp_result= np.array(emp_result)\n",
    "print('----------------MWP1A Magnitude & Sources with Empirical Distribution -')\n",
    "print('----------------Mean Result (95% confidence interval)----------------')\n",
    "print('NAIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,0]),*find_maximum_prob(emp_result[:,0],95)[:2]))\n",
    "print('AIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,1]),*find_maximum_prob(emp_result[:,1],95)[:2]))\n",
    "print('SIS:   {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(emp_result[:,2]),*find_maximum_prob(emp_result[:,2],95)[:2]))\n",
    "print('Total MWP-1A Magnitude:   {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(np.sum(emp_result,axis=1)),*find_maximum_prob(np.sum(emp_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform senario coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -\n",
      "----------------Mean Result (95% confidence interval)----------------\n",
      "NAIS: 12.428 [3.180,19.226]\n",
      "AIS: 2.173 [0.000,8.775]\n",
      "SIS:   3.130 [0.000,5.952]\n",
      "Total MWP-1A Magnitude:   17.732 [14.824,20.371]\n"
     ]
    }
   ],
   "source": [
    "uni_result = []\n",
    "while len(uni_result)<20000:\n",
    "    i = np.random.randint(0,20000,1)[0]\n",
    "    obs = np.array([Tahiti_uni_dis.iloc[i],Barbados_uni_dis.iloc[i],Sunda_uni_dis.iloc[i],HYD_uni_dis.iloc[i],\n",
    "                   NOG_uni_dis.iloc[i],Scot_uni_dis.iloc[i]]) #compile all sample together\n",
    "    opti = list(nnls(np.sqrt(1/np.array(total_uni_std)**2)[:,None] * A,np.sqrt(1/np.array(total_uni_std)**2)*obs)[0]) #non-negative least square\n",
    "    uni_result.append(opti)\n",
    "    \n",
    "uni_result= np.array(uni_result)\n",
    "print('----------------MWP1A Magnitude & Sources with Uniform Distribution -')\n",
    "print('----------------Mean Result (95% confidence interval)----------------')\n",
    "print('NAIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,0]),*find_maximum_prob(uni_result[:,0],95)[:2]))\n",
    "print('AIS: {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,1]),*find_maximum_prob(uni_result[:,1],95)[:2]))\n",
    "print('SIS:   {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(uni_result[:,2]),*find_maximum_prob(uni_result[:,2],95)[:2]))\n",
    "print('Total MWP-1A Magnitude:   {0:5.3f} [{1:5.3f},{2:5.3f}]'.format(np.mean(np.sum(uni_result,axis=1)),*find_maximum_prob(np.sum(uni_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Jackknife Resampling\n",
    "\n",
    "### Empirical senario for coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Without Tahiti -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 11.328 [0.000,18.897]  (6.226)\n",
      "West Antarctic Ice Sheet: 3.083 [0.000,11.932] (4.406)\n",
      "Scandinavian Ice Sheet:   3.499 [0.000,6.511]  (1.994)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Barbados -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 7.603 [0.000,17.945]  (7.262)\n",
      "West Antarctic Ice Sheet: 6.853 [0.000,15.951] (6.272)\n",
      "Scandinavian Ice Sheet:   5.146 [0.000,9.843]  (2.914)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Sunda Shelf -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.607 [0.000,18.667]  (4.947)\n",
      "West Antarctic Ice Sheet: 2.353 [0.000,10.561] (3.589)\n",
      "Scandinavian Ice Sheet:   3.457 [0.000,6.572]  (2.030)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Hydrographer -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 11.772 [0.000,18.330]  (5.264)\n",
      "West Antarctic Ice Sheet: 2.803 [0.000,10.847] (3.811)\n",
      "Scandinavian Ice Sheet:   3.385 [0.000,6.474]  (2.000)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Noggin Pass -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 11.945 [0.000,18.457]  (5.192)\n",
      "West Antarctic Ice Sheet: 2.720 [0.000,10.904] (3.761)\n",
      "Scandinavian Ice Sheet:   3.465 [0.000,6.548]  (2.012)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Scotland -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 13.910 [0.000,21.036]  (6.214)\n",
      "West Antarctic Ice Sheet: 2.925 [0.000,13.394] (4.528)\n",
      "Scandinavian Ice Sheet:   1.441 [0.000,10.936]  (3.716)\n",
      "----------------------------------------------------------------------\n",
      "----------------Overall Jackknife/Original Results --------------------\n",
      "NAIS: 11.527, 12.253  Bias: -0.726 Bias Corrected: 12.979 m\n",
      "AIS: 3.456, 2.551  Bias: 0.905 Bias Corrected: 1.646 m\n",
      "SIS:   3.399, 3.481  Bias: -0.083 Bias Corrected: 3.564 m\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "jack_knife_result =np.zeros([60000,3])\n",
    "site_name = ['Tahiti',\"Barbados\",'Sunda Shelf','Hydrographer','Noggin Pass',\n",
    "            'Scotland']\n",
    "for jack in range(6):\n",
    "    \n",
    "    all_emp_result = [Tahiti_emp_dis,Barbados_emp_dis,Sunda_emp_dis,NOG_emp_dis,\n",
    "                           HYD_emp_dis,Scot_emp_dis]\n",
    "    total_emp_std = [Tahiti_emp_std,Barbados_emp_std,Sunda_emp_std,HYD_emp_std,NOG_emp_std,\n",
    "                         Scot_emp_std]\n",
    "    A_index = ~(np.arange(0,6)==jack)\n",
    "    A_jack = A[A_index]\n",
    "\n",
    "    del all_emp_result[jack] #remove one site's observation\n",
    "    del total_emp_std[jack] #remove one site's standard deviation \n",
    "    emp_test_result = []\n",
    "    all_emp_result = np.array(all_emp_result)\n",
    "    total_emp_std = np.array(total_emp_std)\n",
    "    while len(emp_test_result)<10000:\n",
    "        \n",
    "        i = np.random.randint(0,20000,1)[0]\n",
    "        obs = np.array([Tahiti_emp_dis.iloc[i],Barbados_emp_dis.iloc[i],Sunda_emp_dis.iloc[i],HYD_emp_dis.iloc[i],\n",
    "                       NOG_emp_dis.iloc[i],Scot_emp_dis.iloc[i]])\n",
    "        obs = obs[A_index] \n",
    "        opti = list(nnls(np.sqrt(1/np.array(total_emp_std)**2)[:,None] * A_jack,np.sqrt(1/np.array(total_emp_std)**2)*obs)[0])\n",
    "        emp_test_result.append(opti)\n",
    "       \n",
    "    \n",
    "    emp_test_result= np.array(emp_test_result)\n",
    "    jack_knife_result[jack*10000:(jack+1)*10000,:]=emp_test_result\n",
    "    print('----------------Without {:} -------------------------------------'.format(site_name[jack]))\n",
    "    print('----------------MWP1A Magnitude & Sources with Empirical Distribution -----------')\n",
    "    print('----------------Mean Result [95% confidence interval] (1 std)-----------------')\n",
    "    print('North American Ice Sheet: {0:5.3f} [{1:5.3f},{2:5.3f}]  ({3:5.3f})'.format(np.mean(emp_test_result[:,0]),find_maximum_prob(emp_test_result[:,0],95)[0],find_maximum_prob(emp_test_result[:,0],95)[1],\n",
    "                                                                                   np.std(emp_test_result[:,0])))\n",
    "    print('West Antarctic Ice Sheet: {0:5.3f} [{1:5.3f},{2:5.3f}] ({3:5.3f})'.format(np.mean(emp_test_result[:,1]),find_maximum_prob(emp_test_result[:,1],95)[0],find_maximum_prob(emp_test_result[:,1],95)[1],\n",
    "                                                                                  np.std(emp_test_result[:,1])))\n",
    "    print('Scandinavian Ice Sheet:   {0:5.3f} [{1:5.3f},{2:5.3f}]  ({3:5.3f})'.format(np.mean(emp_test_result[:,2]),find_maximum_prob(emp_test_result[:,2],95)[0],find_maximum_prob(emp_test_result[:,2],95)[1],\n",
    "                                                                                  np.std(emp_test_result[:,2])))\n",
    "    print('----------------------------------------------------------------------')\n",
    "\n",
    "print('----------------Overall Jackknife/Original Results --------------------')\n",
    "print('NAIS: {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,0]),np.mean(emp_result[:,0]),\n",
    "                                                                           np.mean(jack_knife_result[:,0])-np.mean(emp_result[:,0]),\n",
    "                                                                            np.mean(emp_result[:,0])-(np.mean(jack_knife_result[:,0])-np.mean(emp_result[:,0]))))\n",
    "print('AIS: {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,1]),np.mean(emp_result[:,1]),\n",
    "                                                                           np.mean(jack_knife_result[:,1])-np.mean(emp_result[:,1]),\n",
    "                                                                           np.mean(emp_result[:,1])-(np.mean(jack_knife_result[:,1])-np.mean(emp_result[:,1]))))\n",
    "print('SIS:   {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,2]),np.mean(emp_result[:,2]),\n",
    "                                                                          np.mean(jack_knife_result[:,2])-np.mean(emp_result[:,2]),\n",
    "                                                                        np.mean(emp_result[:,2])-( np.mean(jack_knife_result[:,2])-np.mean(emp_result[:,2]))))\n",
    "print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform senario coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Without Tahiti -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 10.538 [0.000,18.309]  (4.193)\n",
      "West Antarctic Ice Sheet: 3.346 [0.000,11.458] (4.279)\n",
      "Scandinavian Ice Sheet:   3.226 [0.000,6.022]  (1.895)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Barbados -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 8.792 [0.000,17.220]  (4.193)\n",
      "West Antarctic Ice Sheet: 5.490 [0.000,15.251] (5.944)\n",
      "Scandinavian Ice Sheet:   4.351 [0.000,8.700]  (2.614)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Sunda Shelf -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.515 [3.149,19.283]  (4.193)\n",
      "West Antarctic Ice Sheet: 2.129 [0.000,8.948] (3.056)\n",
      "Scandinavian Ice Sheet:   3.145 [0.000,5.932]  (1.866)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Hydrographer -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.343 [2.633,19.074]  (4.193)\n",
      "West Antarctic Ice Sheet: 2.205 [0.000,8.919] (3.078)\n",
      "Scandinavian Ice Sheet:   3.133 [0.000,5.953]  (1.875)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Noggin Pass -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 12.304 [2.482,19.272]  (4.193)\n",
      "West Antarctic Ice Sheet: 2.231 [0.000,9.030] (3.107)\n",
      "Scandinavian Ice Sheet:   3.123 [0.000,5.978]  (1.899)\n",
      "----------------------------------------------------------------------\n",
      "----------------Without Scotland -------------------------------------\n",
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------Mean Result [95% confidence interval] (1 std)-----------------\n",
      "North American Ice Sheet: 13.679 [2.982,21.015]  (4.193)\n",
      "West Antarctic Ice Sheet: 2.194 [0.000,9.834] (3.520)\n",
      "Scandinavian Ice Sheet:   1.923 [0.000,11.747]  (3.926)\n",
      "----------------------------------------------------------------------\n",
      "----------------Overall Jackknife/Original Results --------------------\n",
      "NAIS: 11.695, 12.428  Bias: -0.733 Bias Corrected: 13.161 m\n",
      "AIS: 2.932, 2.173  Bias: 0.759 Bias Corrected: 1.415 m\n",
      "SIS:   3.150, 3.130  Bias: 0.020 Bias Corrected: 3.111 m\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "jack_knife_result =np.zeros([60000,3])\n",
    "site_name = ['Tahiti',\"Barbados\",'Sunda Shelf','Hydrographer','Noggin Pass',\n",
    "            'Scotland']\n",
    "for jack in range(6):\n",
    "    \n",
    "    all_uni_result = [Tahiti_uni_dis,Barbados_uni_dis,Sunda_uni_dis,NOG_uni_dis,\n",
    "                           HYD_uni_dis,Scot_uni_dis]\n",
    "    total_uni_std = [Tahiti_uni_std,Barbados_uni_std,Sunda_uni_std,HYD_uni_std,NOG_uni_std,\n",
    "                         Scot_uni_std]\n",
    "    A_index = ~(np.arange(0,6)==jack)\n",
    "    A_jack = A[A_index]\n",
    "\n",
    "    del all_uni_result[jack] #remove one site's observation\n",
    "    del total_uni_std[jack] #remove one site's standard error \n",
    "    uni_test_result = []\n",
    "    all_uni_result = np.array(all_uni_result)\n",
    "    total_uni_std = np.array(total_uni_std)\n",
    "    while len(uni_test_result)<10000:\n",
    "        \n",
    "        i = np.random.randint(0,20000,1)[0]\n",
    "        obs = np.array([Tahiti_uni_dis.iloc[i],Barbados_uni_dis.iloc[i],Sunda_uni_dis.iloc[i],HYD_uni_dis.iloc[i],\n",
    "                       NOG_uni_dis.iloc[i],Scot_uni_dis.iloc[i]])\n",
    "        obs = obs[A_index] \n",
    "        opti = list(nnls(np.sqrt(1/np.array(total_uni_std)**2)[:,None] * A_jack,np.sqrt(1/np.array(total_uni_std)**2)*obs)[0])\n",
    "        uni_test_result.append(opti)\n",
    "       \n",
    "    \n",
    "    uni_test_result= np.array(uni_test_result)\n",
    "    jack_knife_result[jack*10000:(jack+1)*10000,:]=uni_test_result\n",
    "    print('----------------Without {:} -------------------------------------'.format(site_name[jack]))\n",
    "    print('----------------MWP1A Magnitude & Sources with Uniform Distribution -----------')\n",
    "    print('----------------Mean Result [95% confidence interval] (1 std)-----------------')\n",
    "    print('North American Ice Sheet: {0:5.3f} [{1:5.3f},{2:5.3f}]  ({3:5.3f})'.format(np.mean(uni_test_result[:,0]),find_maximum_prob(uni_test_result[:,0],95)[0],find_maximum_prob(uni_test_result[:,0],95)[1],\n",
    "                                                                                   np.std(uni_result[:,0])))\n",
    "    print('West Antarctic Ice Sheet: {0:5.3f} [{1:5.3f},{2:5.3f}] ({3:5.3f})'.format(np.mean(uni_test_result[:,1]),find_maximum_prob(uni_test_result[:,1],95)[0],find_maximum_prob(uni_test_result[:,1],95)[1],\n",
    "                                                                                  np.std(uni_test_result[:,1])))\n",
    "    print('Scandinavian Ice Sheet:   {0:5.3f} [{1:5.3f},{2:5.3f}]  ({3:5.3f})'.format(np.mean(uni_test_result[:,2]),find_maximum_prob(uni_test_result[:,2],95)[0],find_maximum_prob(uni_test_result[:,2],95)[1],\n",
    "                                                                                  np.std(uni_test_result[:,2])))\n",
    "    print('----------------------------------------------------------------------')\n",
    "\n",
    "print('----------------Overall Jackknife/Original Results --------------------')\n",
    "print('NAIS: {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,0]),np.mean(uni_result[:,0]),\n",
    "                                                                           np.mean(jack_knife_result[:,0])-np.mean(uni_result[:,0]),\n",
    "                                                                            np.mean(uni_result[:,0])-(np.mean(jack_knife_result[:,0])-np.mean(uni_result[:,0]))))\n",
    "print('AIS: {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,1]),np.mean(uni_result[:,1]),\n",
    "                                                                           np.mean(jack_knife_result[:,1])-np.mean(uni_result[:,1]),\n",
    "                                                                           np.mean(uni_result[:,1])-(np.mean(jack_knife_result[:,1])-np.mean(uni_result[:,1]))))\n",
    "print('SIS:   {0:5.3f}, {1:5.3f}  Bias: {2:5.3f} Bias Corrected: {3:5.3f} m'.format(np.mean(jack_knife_result[:,2]),np.mean(uni_result[:,2]),\n",
    "                                                                          np.mean(jack_knife_result[:,2])-np.mean(uni_result[:,2]),\n",
    "                                                                        np.mean(uni_result[:,2])-( np.mean(jack_knife_result[:,2])-np.mean(uni_result[:,2]))))\n",
    "print('----------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MWP-1A Sources Inversion with sea-level oscillation limit\n",
    "\n",
    "### Empirical senario for coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------MWP1A Magnitude & Sources with Empirical Distribution -----------\n",
      "----------------Mean Result (95% confidence interval)----------------\n",
      "NAIS: [015.654,15.654]\n",
      "AIS:  [0.000,9.566]\n",
      "SIS:  [2.708,7.405]\n",
      "Total MWP-1A Magnitude:    [14.503,21.803]\n"
     ]
    }
   ],
   "source": [
    "emp_result = []\n",
    "total_emp_std = np.array([Tahiti_emp_std,Barbados_emp_std,Sunda_emp_std,HYD_emp_std,NOG_emp_std,\n",
    "                        Scot_emp_std]) \n",
    "while len(emp_result)<20000:\n",
    "    i = np.random.randint(0,20000,1)[0]\n",
    "    obs = np.array([Tahiti_emp_dis.iloc[i],Barbados_emp_dis.iloc[i],Sunda_emp_dis.iloc[i],HYD_emp_dis.iloc[i],\n",
    "                   NOG_emp_dis.iloc[i],Scot_emp_dis.iloc[i]]) #compile all sample together\n",
    "    opti = list(nnls(np.sqrt(1/np.array(total_emp_std)**2)[:,None] * A,np.sqrt(1/np.array(total_emp_std)**2)*obs)[0]) #non-negative least square\n",
    "    if (opti[0]*0.75+opti[1]*1.09-opti[2]*0.74)<9: # filter out the results producing a sea-level oscillation\n",
    "        emp_result.append(opti)\n",
    "    \n",
    "emp_result= np.array(emp_result)\n",
    "print('----------------MWP1A Magnitude & Sources with Empirical Distribution -----------')\n",
    "print('----------------Mean Result (95% confidence interval)----------------')\n",
    "print('NAIS: [0{1:5.3f},{1:5.3f}]'.format(*find_maximum_prob(emp_result[:,0],95)[:2]))\n",
    "print('AIS:  [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob(emp_result[:,1],95)[:2]))\n",
    "print('SIS:  [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob(emp_result[:,2],95)[:2]))\n",
    "print('Total MWP-1A Magnitude:    [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob(np.sum(emp_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform senario coral records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------MWP1A Magnitude & Sources with Uniform Distribution -----------\n",
      "----------------95% confidence interval----------------\n",
      "NAIS: [016.114,16.114]\n",
      "AIS:  [0.000,7.392]\n",
      "SIS:  [2.595,6.578]\n",
      "Total MWP-1A Magnitude: [15.099,20.484]\n"
     ]
    }
   ],
   "source": [
    "uni_result = []\n",
    "total_uni_std = np.array([Tahiti_uni_std,Barbados_uni_std,Sunda_uni_std,HYD_uni_std,NOG_uni_std,\n",
    "                        Scot_uni_std])\n",
    "while len(uni_result)<20000:\n",
    "    i = np.random.randint(0,20000,1)[0]\n",
    "    obs = np.array([Tahiti_uni_dis.iloc[i],Barbados_uni_dis.iloc[i],Sunda_uni_dis.iloc[i],HYD_uni_dis.iloc[i],\n",
    "                   NOG_uni_dis.iloc[i],Scot_uni_dis.iloc[i]]) #compile all sample together\n",
    "    opti = list(nnls(np.sqrt(1/np.array(total_uni_std)**2)[:,None] * A,np.sqrt(1/np.array(total_uni_std)**2)*obs)[0]) #non-negative least square\n",
    "    if (opti[0]*0.75+opti[1]*1.09-opti[2]*0.74)<9: # filter out the results producing a sea-level oscillation\n",
    "\n",
    "        uni_result.append(opti)\n",
    "    \n",
    "uni_result= np.array(uni_result)\n",
    "print('----------------MWP1A Magnitude & Sources with Uniform Distribution -----------')\n",
    "print('----------------95% confidence interval----------------')\n",
    "print('NAIS: [0{1:5.3f},{1:5.3f}]'.format(*find_maximum_prob(uni_result[:,0],95)[:2]))\n",
    "print('AIS:  [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob(uni_result[:,1],95)[:2]))\n",
    "print('SIS:  [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob(uni_result[:,2],95)[:2]))\n",
    "print('Total MWP-1A Magnitude: [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob(np.sum(uni_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Averaged results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------Averaged MWP1A Magnitude & Sources -----------\n",
      "----------------Median Result (95% confidence interval)----------------\n",
      "NAIS: [5.660,15.467]\n",
      "AIS:  [0.000,5.881]\n",
      "SIS:  [3.244,6.357]\n",
      "Total MWP-1A Magnitude: [15.099,20.484]\n"
     ]
    }
   ],
   "source": [
    "print('----------------Averaged MWP1A Magnitude & Sources -----------')\n",
    "print('----------------Median Result (95% confidence interval)----------------')\n",
    "print('NAIS: [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob((uni_result[:,0]+emp_result[:,0])/2,95)[:2]))\n",
    "print('AIS:  [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob((uni_result[:,1]+emp_result[:,1])/2,95)[:2]))\n",
    "print('SIS:  [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob((uni_result[:,2]+emp_result[:,2])/2,95)[:2]))\n",
    "print('Total MWP-1A Magnitude: [{0:5.3f},{1:5.3f}]'.format(*find_maximum_prob(np.sum(uni_result,axis=1),95)[:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final notes:\n",
    "\n",
    "- Due to the different ramdom seeds, this code can possibly result in a slightly different results comparing to the results shown in our manuscript\n",
    "- If you have any questions, feedbacks or comments, please feel free to contact [the corresponding author](yc-lin.com): yucheng.lin@durham.ac.uk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
